# KV Cache Integration Guide\n\n## Quick Start\n\n### 1. Initialize Cache\n\n```cpp\n#include \"cortexstream/kv_cache.h\"\n\nusing namespace cortexstream;\n\n// Configure based on model\nconst size_t numLayers = 32;\nconst size_t numHeads = 32;\nconst size_t headDim = 64;\nconst size_t maxTotalTokens = 32768;  // Total tokens across all sequences\nconst size_t blockSize = 16;           // Tokens per block\n\nKVCache cache(\n    numLayers,\n    numHeads,\n    headDim,\n    maxTotalTokens,\n    blockSize\n);\n\ncache.warmup();  // Touch pages for instant allocation\n```\n\n### 2. Allocate for Sequence (Prefill)\n\n```cpp\nconst std::string requestId = \"user_123\";\nconst std::vector<int> promptTokens = {...};  // Input tokens\nconst int promptLength = promptTokens.size();\n\nbool success = cache.allocateFor(requestId, promptLength);\n\nif (!success) {\n    // OOM: Out of KV cache memory\n    // Options:\n    // 1. Reject request\n    // 2. Reduce maxTokens\n    // 3. Wait for other sequences to complete\n    request->setState(RequestState::Failed);\n    return;\n}\n\n// Allocation succeeded, ready for prefill forward pass\n```\n\n### 3. Get Tensor Views (Forward Pass)\n\n```cpp\n// In ModelBackend::prefill()\n\nfor (int layer = 0; layer < numLayers; ++layer) {\n    // Get zero-copy views of KV cache\n    Tensor k_view = cache.getKView(requestId, layer);\n    Tensor v_view = cache.getVView(requestId, layer);\n    \n    if (!k_view.valid) {\n        throw std::runtime_error(\"KV view invalid\");\n    }\n    \n    // Pass to MLX attention function\n    // MLX operates on the arena memory directly—no copying!\n    mlx::array K = mlx::array(\n        k_view.data,\n        {(int)k_view.shape[0], (int)k_view.shape[1], (int)k_view.shape[2]}\n    );\n    mlx::array V = mlx::array(\n        v_view.data,\n        {(int)v_view.shape[0], (int)v_view.shape[1], (int)v_view.shape[2]}\n    );\n    \n    // Attention computation\n    auto [attn_output, ...] = attention_layer(\n        queries, K, V, ...\n    );\n}\n```\n\n### 4. Append Tokens (Decode Loop)\n\n```cpp\n// In InferenceEngine::processDecode()\n\nfor (const auto& request : decodeBatch) {\n    const std::string& requestId = request->getId();\n    \n    // Sample next token\n    int token = sampler.sampleToken(logits, ...);\n    \n    // Add to request\n    request->addToken(token);\n    \n    // Update cache position\n    if (!cache.appendToken(requestId)) {\n        // Sequence exceeded allocated blocks\n        // Options:\n        // 1. Stop generation (user configurable)\n        // 2. Evict LRU sequence for more space\n        // 3. Return error to client\n        request->setState(RequestState::Failed);\n        continue;\n    }\n    \n    // Check if request complete\n    if (request->getGeneratedLength() >= request->getMaxTokens()) {\n        request->setState(RequestState::Finished);\n    }\n}\n```\n\n### 5. Cleanup (Request Complete)\n\n```cpp\n// In InferenceEngine after request finishes\n\ncache.freeFor(request->getId());\n\n// Blocks immediately available for reuse\n// No per-block cleanup needed (zero-copy design)\n```\n\n---\n\n## Monitoring\n\n### Real-Time Statistics\n\n```cpp\n// In your monitoring loop (e.g., every second)\n\nint numSeqs = cache.getNumAllocatedSequences();\nsize_t allocatedMB = cache.getTotalAllocated() / 1024 / 1024;\nsize_t freeMB = cache.getTotalFree() / 1024 / 1024;\nfloat fragmentation = cache.getFragmentation();\n\nstd::cout << std::format(\n    \"KV Cache: {} seqs, {}/{} MB, frag={:.2f}\",\n    numSeqs, allocatedMB, allocatedMB + freeMB, fragmentation\n);\n\nif (cache.isFull()) {\n    std::cerr << \"WARNING: KV cache full!\\n\";\n    // Consider rejecting new requests\n}\n```\n\n### Debug Dump\n\n```cpp\n// On request, or periodically\n\ncache.dumpCacheStats(std::cout);\n// Output:\n// === KVCache Statistics ===\n// Configuration:\n//   Layers: 32, Heads: 32, HeadDim: 64\n//   BlockSize: 16, TotalBlocks: 2048\n// Allocation State:\n//   Allocated sequences: 5\n//   Total allocated: 1024.00 MB\n//   Total free: 7168.00 MB\n//   Fragmentation: 0.05\n// Sequences:\n//   req_001: 512/512 tokens, blocks [0, +32]\n//   req_002: 256/512 tokens, blocks [32, +32]\n//   ...\n```\n\n---\n\n## Memory Calculation\n\n### Arena Size\n\n```cpp\nsize_t bytesPerElement = sizeof(float);  // 4 bytes\nsize_t elementsPerBlock = numHeads * blockSize * headDim;\nsize_t elementsPerLayer = totalBlocks * elementsPerBlock;\nsize_t totalElements = numLayers * elementsPerLayer * 2;  // K and V\n\nsize_t totalBytes = totalElements * bytesPerElement;\nsize_t totalMB = totalBytes / 1024 / 1024;\nsize_t totalGB = totalMB / 1024;\n```\n\n### Example: 7B LLaMA on 40GB GPU\n\n```\nConfiguration:\n  Model: LLaMA-7B\n  numLayers = 32\n  numHeads = 32\n  headDim = 64\n  blockSize = 16\n  maxTotalTokens = 32768\n  \nCalculation:\n  totalBlocks = (32768 + 16 - 1) / 16 = 2048\n  elementsPerBlock = 32 * 16 * 64 = 32,768\n  elementsPerLayer = 2048 * 32,768 = 67,108,864\n  totalElements = 32 * 67,108,864 * 2 = 4,294,967,296 (4.3B)\n  \n  totalBytes = 4.3B * 4 = 17.2 GB\n  \nGPU Budget (40 GB total):\n  Model weights: ~8 GB\n  KV cache: 17.2 GB\n  Activations/batch: ~4 GB\n  Overhead: ~2 GB\n  Remaining: ~8.8 GB\n```\n\n**Practical:** Tune `maxTotalTokens` and `blockSize` based on available GPU memory.\n\n---\n\n## Tuning Parameters\n\n### Block Size\n\n**Smaller blocks (blockSize=8):**\n- ✅ Lower latency variance (write position changes per token)\n- ✅ Finer control for partial cache eviction\n- ❌ More blocks needed (higher allocator overhead)\n\n**Larger blocks (blockSize=32):**\n- ✅ Fewer blocks (lower metadata overhead)\n- ✅ Better GPU memory coalescing\n- ❌ Higher latency variance per block boundary\n\n**Recommendation:** 16-32 for interactive latency, 64+ for batch throughput.\n\n### Token Limit Strategy\n\n**Fixed maximum (current):**\n```cpp\nbool success = cache.allocateFor(requestId, promptLength);\nint maxAllowed = blocksNeeded * blockSize;\n```\nAllocates fixed blocks based on initial prompt.\nRejects if append exceeds maxAllowed.\n\n**Dynamic (future):**\n```cpp\ncache.allocateFor(requestId, promptLength, maxTokens=2048);\n// Allocates minimum blocks for prompt\n// Auto-extends up to 2048 tokens\n```\n\n### OOM Handling\n\n**Strategy 1: Reject**\n```cpp\nif (!cache.allocateFor(requestId, promptLength)) {\n    request->setState(RequestState::Failed);\n    return;\n}\n```\nFastest response, loses request.\n\n**Strategy 2: Reduce max tokens**\n```cpp\nint maxTokens = request->getMaxTokens();\nwhile (maxTokens > 0) {\n    if (cache.allocateFor(requestId, promptLength, maxTokens)) {\n        break;\n    }\n    maxTokens /= 2;  // Reduce capacity\n}\n```\nAccepts request with shorter output.\n\n**Strategy 3: Wait**\n```cpp\nwhile (!cache.allocateFor(requestId, promptLength)) {\n    sleep(10ms);  // Wait for other requests to complete\n    if (waitTime > timeout) {\n        request->setState(RequestState::Failed);\n        break;\n    }\n}\n```\nQueues request, increases latency.\n\n---\n\n## Debugging\n\n### Fragmentation Issues\n\n```cpp\nfloat frag = cache.getFragmentation();\n\nif (frag > 0.5f) {\n    std::cerr << \"High fragmentation detected!\\n\";\n    std::cerr << \"Consider: \\n\";\n    std::cerr << \"  1. Increase blockSize\\n\";\n    std::cerr << \"  2. Implement buddy allocator\\n\";\n    std::cerr << \"  3. Add paging support\\n\";\n}\n```\n\n**Note:** Current design has zero fragmentation for contiguous allocation, but future paging might introduce scatter.\n\n### Allocation Failures\n\n```cpp\nif (!cache.allocateFor(requestId, promptLength)) {\n    std::cerr << std::format(\n        \"Allocation failed: requested {} tokens, {} tokens available\\n\",\n        promptLength,\n        cache.getTotalFree() / (numHeads_ * blockSize_ * headDim_ * sizeof(float))\n    );\n}\n```\n\n### Block Map Visualization\n\n```cpp\nallocator_->dumpBlockMap(std::cout);\n// Output:\n// KVBlockAllocator State:\n//   Total blocks: 2048\n//   Used: 128 Free: 1920\n//   Fragmentation: 0.00\n//   Block map (. = free, X = used):\n//     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX................\n//     ........................XXXXXXXXXXXX................\n```\n\n---\n\n## Error Handling\n\n### Null Views\n\n```cpp\nauto k_view = cache.getKView(invalidRequestId, 0);\n\nif (!k_view.valid) {\n    // Sequence not found\n    // Options:\n    // 1. Throw exception\n    // 2. Use default cache (e.g., zeros)\n    // 3. Skip layer\n}\n```\n\n### Token Append Failure\n\n```cpp\nif (!cache.appendToken(requestId)) {\n    // Exceeded allocation capacity\n    // Request must stop generation\n    request->setState(RequestState::Failed);\n    \n    // Optionally log\n    std::cerr << std::format(\n        \"Sequence {} exhausted {} token slots\\n\",\n        requestId,\n        cache.usedTokens(requestId)\n    );\n}\n```\n\n### Double Free\n\n```cpp\ncache.freeFor(requestId);  // First free\ncache.freeFor(requestId);  // Second free—safe (no-op)\n\n// Implementation ignores invalid frees gracefully\n```\n\n---\n\n## Performance Checklist\n\n- [ ] Arena preallocated at startup (no malloc during inference)\n- [ ] Tensor views use direct pointers (no allocation per-token)\n- [ ] Block size tuned for model (16-32 typical)\n- [ ] Fragmentation monitored (< 0.2 acceptable)\n- [ ] OOM strategy configured (reject, reduce, wait)\n- [ ] Memory budget verified (KV + model + activations < GPU memory)\n- [ ] Monitoring instrumented (log to metrics system)\n- [ ] Error paths tested (allocation failure, exceed capacity)\n\n---\n\n## Next Steps\n\n### For Integration\n1. Set configuration constants (numLayers, numHeads, etc.)\n2. Create cache instance in Engine ctor\n3. Call allocateFor() in Scheduler prefill path\n4. Use getKView/getVView in ModelBackend forward\n5. Call appendToken() in decode loop\n6. Call freeFor() in cleanup\n7. Monitor cache stats in telemetry\n\n### For Production\n1. Implement per-layer slicing for better GPU memory patterns\n2. Add metrics exporting (Prometheus, etc.)\n3. Implement allocation queue for concurrent requests\n4. Test with real workloads (varying sequence lengths)\n5. Profile GPU memory utilization with MLX\n6. Consider buddy allocator upgrade if fragmentation emerges\n\n---\n\n## References\n\n- [KV Cache Design Doc](KV_CACHE_DESIGN.md)\n- [Triton Comparison](TRITON_COMPARISON.md)\n- [API Reference](api_reference.md#kvcache)\n- [Source Code](../src/cache/kv_cache.cpp)\n